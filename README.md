# Web Performance Scanner

A modular Python CLI tool that batch-scans website routes using the **Google PageSpeed Insights API** and produces colour-coded performance reports with averaged metrics.

---

## Table of Contents

- [Features](#features)
- [Architecture](#architecture)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Configuration](#configuration)
  - [API Key](#api-key)
  - [Environment Variables](#environment-variables)
  - [URL Input File](#url-input-file)
- [Usage](#usage)
  - [Basic Run](#basic-run)
  - [CLI Arguments](#cli-arguments)
  - [Examples](#examples)
- [Output](#output)
  - [Terminal Table](#terminal-table)
  - [CSV Export](#csv-export)
  - [Score Colour Coding](#score-colour-coding)
- [Module Reference](#module-reference)
  - [main.py](#mainpy)
  - [reader.py](#readerpy)
  - [scanner.py](#scannerpy)
  - [reporter.py](#reporterpy)
- [Performance & Concurrency](#performance--concurrency)
- [Error Handling](#error-handling)
- [Troubleshooting](#troubleshooting)
- [Project Structure](#project-structure)
- [License](#license)

---

## Features

- **Batch scanning** â€” Analyse hundreds of URLs in a single run.
- **Concurrent API calls** â€” Uses a thread pool (`ThreadPoolExecutor`) so every URL gets its own "channel"; scans that previously took ~2 hours now finish in ~10â€“15 minutes.
- **Dual strategy** â€” Every URL is tested for both **mobile** and **desktop**.
- **Four Lighthouse categories** â€” Performance, Accessibility, Best Practices, and SEO.
- **Flexible input** â€” The CSV file accepts full URLs (`https://â€¦`) *or* bare route paths (`/about`) that are combined with a configurable base domain.
- **Automatic deduplication** â€” Duplicate URLs are removed before scanning.
- **Colour-coded terminal output** â€” Powered by [Rich](https://github.com/Textualize/rich); scores are green (â‰¥ 90), yellow (50â€“89), or red (< 50).
- **Averages row** â€” Overall averages are computed with [pandas](https://pandas.pydata.org/) and displayed at the bottom of the table.
- **CSV export** â€” Full results (individual + averages) are written to `results.csv`.
- **Secure configuration** â€” API key loaded from a `.env` file via `python-dotenv`.
- **Graceful error handling** â€” File-not-found, HTTP errors, timeouts, and malformed API responses are all caught and reported cleanly.

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       main.py                           â”‚
â”‚              CLI entry point & orchestrator              â”‚
â”‚  Loads .env â†’ Parses args â†’ Coordinates modules below   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚              â”‚                  â”‚
         â–¼              â–¼                  â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ reader.pyâ”‚  â”‚ scanner.py  â”‚   â”‚ reporter.py â”‚
   â”‚          â”‚  â”‚             â”‚   â”‚             â”‚
   â”‚ CSV      â”‚  â”‚ PageSpeed   â”‚   â”‚ pandas      â”‚
   â”‚ parsing  â”‚  â”‚ API calls   â”‚   â”‚ aggregation â”‚
   â”‚ & URL    â”‚  â”‚ (concurrent â”‚   â”‚ + Rich      â”‚
   â”‚ building â”‚  â”‚  threads)   â”‚   â”‚ table +     â”‚
   â”‚          â”‚  â”‚             â”‚   â”‚ CSV export  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Data flow:**

1. `reader.py` reads `urls.csv` and separates full URLs from route paths.
2. Route paths are combined with the base domain to produce full URLs.
3. `scanner.py` dispatches all URL Ã— strategy combinations to a thread pool and collects Lighthouse scores.
4. `reporter.py` aggregates the results, prints a colour-coded table, and exports to CSV.

---

## Prerequisites

- **Python 3.10+** (tested with Python 3.13)
- A **Google PageSpeed Insights API key** â€” free to obtain from the [Google Cloud Console](https://developers.google.com/speed/docs/insights/v5/get-started)

---

## Installation

### 1. Clone the repository

```bash
git clone <repository-url>
cd webperformancescanner
```

### 2. Create a virtual environment

```bash
python -m venv venv
```

### 3. Activate the virtual environment

**Windows (PowerShell):**
```powershell
.\venv\Scripts\Activate.ps1
```

**Windows (cmd):**
```cmd
.\venv\Scripts\activate.bat
```

**Linux / macOS:**
```bash
source venv/bin/activate
```

### 4. Install dependencies

```bash
pip install -r requirements.txt
```

**Dependencies:**

| Package        | Purpose                                 |
|----------------|-----------------------------------------|
| `requests`     | HTTP client for PageSpeed Insights API  |
| `pandas`       | Data aggregation & averaging            |
| `python-dotenv`| Load API key from `.env` file           |
| `rich`         | Colour-coded terminal tables & progress |

> `csv`, `argparse`, `threading`, `concurrent.futures` are Python standard library modules â€” no installation required.

---

## Configuration

### API Key

1. Visit the [Google PageSpeed Insights â€“ Get Started](https://developers.google.com/speed/docs/insights/v5/get-started) page.
2. Create or select a Google Cloud project and enable the **PageSpeed Insights API**.
3. Generate an API key from the **Credentials** panel.
4. Paste the key into your `.env` file (see below).

### Environment Variables

Create a `.env` file in the project root (already git-ignored):

```dotenv
# Required â€” your Google API key
API_KEY=AIzaSy__your_key_here__

# Optional â€” default base domain for route paths
BASE_URL=https://example.com

# Optional â€” delay between API requests (seconds, default: 2)
REQUEST_DELAY=2
```

| Variable        | Required | Default              | Description                                      |
|-----------------|----------|----------------------|--------------------------------------------------|
| `API_KEY`       | **Yes**  | â€”                    | Google PageSpeed Insights API key                |
| `BASE_URL`      | No*      | `https://example.com`| Base domain prepended to route paths             |
| `REQUEST_DELAY` | No       | `2`                  | Delay between sequential requests (legacy param) |

> \* `BASE_URL` is only required if your `urls.csv` contains bare route paths (e.g. `/about`). If the CSV has full URLs, it is not needed.

### URL Input File

Create a file called `urls.csv` in the project root. It must have a **header row** and one entry per line.

**Option A â€” Full URLs (recommended for multi-domain scans):**

```csv
URL
https://example.com/
https://example.com/about
https://example.com/pricing
https://another-site.com/
```

**Option B â€” Route paths (combined with `BASE_URL`):**

```csv
route
/
/about
/pricing
/contact
```

**Option C â€” Mixed (both formats in one file):**

```csv
URL
https://other-domain.com/page
/about
/pricing
```

Full URLs are used as-is; route paths have the base domain prepended.

---

## Usage

### Basic Run

```bash
python main.py
```

This uses the `API_KEY` and `BASE_URL` from `.env`, reads from `urls.csv`, runs 10 concurrent workers, and exports to `results.csv`.

### CLI Arguments

| Argument       | Type   | Default       | Description                                                  |
|----------------|--------|---------------|--------------------------------------------------------------|
| `--base-url`   | string | `.env` value  | Base domain to prepend to route paths                        |
| `--csv`        | string | `urls.csv`    | Path to the input CSV file                                   |
| `--delay`      | float  | `2.0`         | Legacy delay parameter (kept for compatibility)              |
| `--output`     | string | `results.csv` | Path for the exported results file                           |
| `--workers`    | int    | `10`          | Number of concurrent threads ("channels") for parallel scans |

### Examples

```bash
# Scan with defaults (10 workers, urls.csv, results.csv)
python main.py

# Override base URL from the command line
python main.py --base-url https://mysite.com

# Use a different input file and 20 parallel workers
python main.py --csv my_routes.csv --workers 20

# Export results to a custom file
python main.py --output report_feb2026.csv

# Conservative mode (fewer workers to avoid rate limits)
python main.py --workers 5

# Full example with all options
python main.py --base-url https://mysite.com --csv routes.csv --workers 15 --output scan.csv
```

---

## Output

### Terminal Table

The scanner prints a colour-coded table using the Rich library:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ URL                              â”‚ Strategy â”‚ Performance â”‚ Accessibility â”‚ Best Practices â”‚ SEO â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ https://example.com/             â”‚ Mobile   â”‚     45      â”‚      92       â”‚       87       â”‚  91 â”‚
â”‚ https://example.com/             â”‚ Desktop  â”‚     78      â”‚      92       â”‚       87       â”‚  91 â”‚
â”‚ https://example.com/about        â”‚ Mobile   â”‚     62      â”‚      95       â”‚       91       â”‚  89 â”‚
â”‚ https://example.com/about        â”‚ Desktop  â”‚     91      â”‚      95       â”‚       91       â”‚  89 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ AVERAGE                          â”‚   ALL    â”‚    69.0     â”‚     93.5      â”‚      89.0      â”‚90.0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
```

### CSV Export

The `results.csv` file contains all individual results plus an `AVERAGE` row:

```csv
url,strategy,performance,accessibility,best-practices,seo
https://example.com/,mobile,45,92,87,91
https://example.com/,desktop,78,92,87,91
https://example.com/about,mobile,62,95,91,89
https://example.com/about,desktop,91,95,91,89
AVERAGE,ALL,69.0,93.5,89.0,90.0
```

### Score Colour Coding

| Colour     | Score Range | Meaning          |
|------------|-------------|------------------|
| ðŸŸ¢ Green   | 90 â€“ 100    | Good             |
| ðŸŸ¡ Yellow  | 50 â€“ 89     | Needs improvement|
| ðŸ”´ Red     | 0 â€“ 49      | Poor             |
| âš« Dim/Grey | N/A         | Scan failed      |

These thresholds match [Google's official Lighthouse scoring](https://developer.chrome.com/docs/lighthouse/performance/performance-scoring/).

---

## Module Reference

### main.py

**Role:** CLI entry point and orchestrator.

| Function        | Description                                        |
|-----------------|----------------------------------------------------|
| `_load_env()`   | Loads variables from `.env` via `python-dotenv`    |
| `_parse_args()` | Parses CLI arguments with `argparse`               |
| `main()`        | Orchestrates the full pipeline: read â†’ scan â†’ report |

**Pipeline steps:**
1. Load `.env` and parse CLI arguments
2. Validate `API_KEY` (abort if missing)
3. Resolve `BASE_URL` (from CLI or `.env`)
4. Read URLs from CSV via `reader.read_urls()`
5. Build full URLs from route paths via `reader.build_full_urls()`
6. Deduplicate URLs
7. Scan all URLs concurrently via `scanner.scan_urls()`
8. Aggregate and display via `reporter.*`
9. Export to CSV

---

### reader.py

**Role:** CSV parsing and URL construction.

| Function                         | Description                                                 |
|----------------------------------|-------------------------------------------------------------|
| `_is_full_url(value)`            | Returns `True` if the value has an `http(s)://` scheme      |
| `read_urls(csv_path)`            | Reads CSV and separates full URLs from route paths          |
| `build_full_urls(base_url, routes)` | Prepends a base domain to a list of route paths          |

**Input format:** Single-column CSV with a header row. Each row is either a full URL or a bare route path.

**Error handling:**
- File not found â†’ prints error, exits with code 1
- CSV parse error â†’ prints error, exits with code 1
- Empty file â†’ prints error, exits with code 1
- Route path missing leading `/` â†’ auto-prepended with a warning

---

### scanner.py

**Role:** Google PageSpeed Insights API interaction with concurrent execution.

| Function                              | Description                                              |
|---------------------------------------|----------------------------------------------------------|
| `_fetch_pagespeed(url, strategy, key)`| Single API request; returns JSON or `None` on failure    |
| `_extract_scores(data)`              | Extracts 4 category scores (0â€“100) from API response     |
| `_scan_single(url, strategy, key)`   | Unit of work for one URL + strategy (submitted to thread pool) |
| `scan_urls(urls, key, delay, max_workers)` | Dispatches all jobs concurrently and collects results |

**API details:**
- Endpoint: `https://www.googleapis.com/pagespeedonline/v5/runPagespeed`
- Strategies: `mobile`, `desktop`
- Categories: `performance`, `accessibility`, `best-practices`, `seo`
- Timeout: 120 seconds per request

**Concurrency model:**
- Uses `concurrent.futures.ThreadPoolExecutor` with configurable `max_workers`
- All URL Ã— strategy pairs are submitted simultaneously
- Results are collected via `as_completed()` for real-time progress updates
- Thread-safe result collection via `threading.Lock`
- Results are sorted back into original URL order after collection

---

### reporter.py

**Role:** Data aggregation, terminal display, and CSV export.

| Function                              | Description                                            |
|---------------------------------------|--------------------------------------------------------|
| `_score_color(score)`                 | Returns a Rich colour name based on score thresholds   |
| `_format_score(score)`                | Returns a coloured `rich.Text` object                  |
| `build_dataframe(results)`            | Converts result dicts to a `pandas.DataFrame`          |
| `compute_averages(df)`               | Calculates mean score per category (rounded to 1 d.p.) |
| `print_results_table(df, averages)`   | Prints the colour-coded Rich table to the terminal     |
| `export_csv(df, averages, path)`      | Writes results + averages row to a CSV file            |

---

## Performance & Concurrency

The scanner uses a **thread pool** to run API calls in parallel, dramatically reducing wall-clock time.

| Scenario             | URLs | API Calls | Sequential Time | 10 Workers  | 20 Workers |
|----------------------|------|-----------|-----------------|-------------|------------|
| Small site           | 10   | 20        | ~7 min          | ~2 min      | ~1 min     |
| Medium site          | 50   | 100       | ~35 min         | ~5 min      | ~3 min     |
| Large site (yours)   | 189  | 378       | ~2+ hours       | ~10â€“15 min  | ~5â€“8 min   |

**How it works:**

1. Each URL Ã— strategy pair becomes a "job".
2. Jobs are submitted to a `ThreadPoolExecutor` with `max_workers` threads.
3. Up to `max_workers` API calls run simultaneously â€” each on its own "channel".
4. As each job completes, the progress bar updates in real time.
5. Once all jobs finish, results are sorted and passed to the reporter.

**Tuning `--workers`:**

| Workers | Trade-off                                                    |
|---------|--------------------------------------------------------------|
| `5`     | Conservative â€” unlikely to hit rate limits, slower overall   |
| `10`    | Default â€” good balance of speed and API courtesy             |
| `15â€“20` | Aggressive â€” faster, but may trigger `429 Too Many Requests` |
| `25+`   | Not recommended unless you have a high API quota             |

> **Tip:** If you see `HTTP Error 429` in the output, reduce workers: `--workers 5`

---

## Error Handling

The tool handles errors gracefully at every stage:

| Error                      | Behaviour                                                  |
|----------------------------|------------------------------------------------------------|
| Missing `.env` / API key   | Prints clear message, exits with code 1                    |
| Missing `urls.csv`         | Prints file-not-found message, exits with code 1           |
| Empty CSV / no valid rows  | Prints "no entries found" message, exits with code 1       |
| HTTP 4xx / 5xx from API    | Logs the error, records `N/A` scores, continues scanning   |
| Connection error           | Logs the error, records `N/A` scores, continues scanning   |
| Request timeout (>120s)    | Logs the error, records `N/A` scores, continues scanning   |
| Malformed API response     | Missing categories are recorded as `None` / `N/A`         |
| Unexpected thread exception| Caught, logged, recorded as `N/A`, scan continues          |

The scanner **never crashes mid-run** â€” failed URLs are recorded with `N/A` scores so you can identify and re-scan them.

---

## Troubleshooting

### "No valid API_KEY found"
Ensure your `.env` file exists in the project root and contains `API_KEY=your_actual_key` (not the placeholder).

### "CSV file not found"
The default input path is `urls.csv` in the current working directory. Use `--csv path/to/file.csv` to specify a different location.

### HTTP 429 â€” Too Many Requests
The Google API has rate limits. Reduce the number of concurrent workers:
```bash
python main.py --workers 5
```

### HTTP 400 â€” Bad Request
Usually means a URL is malformed. Check your `urls.csv` for typos or invalid URLs.

### All scores show N/A
- Verify your API key is valid and has the PageSpeed Insights API enabled.
- Check your internet connection.
- Try scanning a single known-good URL to isolate the issue.

### Script runs but no output file
Ensure you have write permissions in the current directory. Check the `--output` path.

---

## Project Structure

```
webperformancescanner/
â”œâ”€â”€ .env                 # API key & config (git-ignored)
â”œâ”€â”€ .gitignore           # Ignores venv, .env, results, __pycache__
â”œâ”€â”€ main.py              # CLI entry point & orchestrator
â”œâ”€â”€ reader.py            # CSV parsing & URL construction
â”œâ”€â”€ scanner.py           # PageSpeed API calls (concurrent)
â”œâ”€â”€ reporter.py          # pandas aggregation, Rich table, CSV export
â”œâ”€â”€ urls.csv             # Input: URLs or route paths to scan
â”œâ”€â”€ results.csv          # Output: scan results (git-ignored)
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ README.md            # This documentation
â””â”€â”€ venv/                # Virtual environment (git-ignored)
```

---

## License

This project is proprietary to Oracom. All rights reserved.
